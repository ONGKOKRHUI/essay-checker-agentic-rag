{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafe57d7",
   "metadata": {},
   "source": [
    "### Fact Checker\n",
    "- receives a json of facts, loop through them\n",
    "- for each fact, queries the knowledge base tool to check\n",
    "- if not found, uses web search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fdd3e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install dependencies\n",
    "import requests\n",
    "from langchain.agents import create_agent\n",
    "from langchain.messages import HumanMessage\n",
    "from langchain.tools import tool\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import List, Literal, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee108774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Access the variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "SILICON_FLOW_BASE_URL = os.getenv(\"SILICON_FLOW_BASE_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a2dcd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set paths\n",
    "FACTS_JSONL_PATH = \"C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\processed\\\\extracted_facts.jsonl\"\n",
    "ALLOWED_DOMAINS = \"\" # can set to wikipedia domain if want to limit\n",
    "FACT_CHECKING_OUTPUT_PATH = \"C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\processed\\\\fact_checking_output.json\"\n",
    "DOCUMENT_FOLDER_PATH = \"C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\knowledge_base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "675b5186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2024', 'creator': 'Microsoft® Word 2024', 'creationdate': '2025-12-20T19:55:41+08:00', 'author': 'hp404sk7@outlook.com', 'moddate': '2025-12-20T19:55:41+08:00', 'source': 'C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\knowledge_base\\\\AI_and_assessment_in_higher_education.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1'}, page_content='AI and assessment in higher education \\nhttps://www.timeshighereducation.com/campus/ai-and-assessment-higher-\\neducation  \\nThreat or opportunity? Advice for using, managing and embedding artificial intelligence \\nin university assessment, skills development and task design \\nNo sooner had generative AI (GenAI) tools, such as ChatGPT, ignited fears in universities \\nabout risk to assessment practices and academic integrity, than academics began \\nworking out how to embrace it to save time and enrich student skills such as critical \\nthinking and analysis. This has required consideration of not only how to use artificial \\nintelligence (AI) in future university assessment but also a rethink of past exam, \\nassignment and evaluation practices. This diverse collection of resources includes \\nadvice on how to engineer prompts, use AI for authentic assessment design, whether to \\nlean into AI-detection tools, how to build digital literacy and AI’s role in developing soft \\nskills in lifelong learning. \\nGet started with 25 applications of ChatGPT and generative AI in learning and \\nassessment shared in the form of prompts by Seb Dianati and Suman Laudari of \\nCharles Darwin University. \\nHow AI can affect formative and summative assessment design \\nAs AI technologies become ubiquitous, educators must consider how to design \\nassignments that work with these tools in productive ways to aid learning and AI \\nliteracy. These resources explore practical ways to incorporate AI into task design and \\nassessment strategies, taking into account students’ differing skill levels.  \\nHow students’ GenAI skills and reflection affect assignment instructions: The \\nability to use GenAI is akin to time management or other learning skills that require \\npractice. Here, Vincent Spezzo and Ilya Gokhman from Georgia Tech’s Center for 21st \\nCentury Universities offer tips to make sure lecturers’ instructions make sense to \\nstudents with differing levels of AI experience. \\nAI and assessment redesign: a four-step process: If GenAI tools mean institutions \\ncan no longer assure the integrity of individual assessments, the sector must focus on \\nassuring the integrity of overall awards, write Samuel Doherty and Steven Warburton of \\nthe University of Newcastle, Australia. \\nDesigning assessments with generative AI in mind: The proliferation of AI requires a \\nbalance between thoughtfully mitigating and responsibly promoting students’ use of the \\nnew tools. Kate Crane of Dalhousie University offers four strategies to help faculty chart \\na path forward. \\nAI as a learning aid for critical thinking'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2024', 'creator': 'Microsoft® Word 2024', 'creationdate': '2025-12-20T19:55:41+08:00', 'author': 'hp404sk7@outlook.com', 'moddate': '2025-12-20T19:55:41+08:00', 'source': 'C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\knowledge_base\\\\AI_and_assessment_in_higher_education.pdf', 'total_pages': 4, 'page': 1, 'page_label': '2'}, page_content=\"Critical thinking is a future-proof skill in high demand from employers. The arrival of \\nartificial intelligence, specifically GenAI, makes honing critical thinking among students \\nand academics even more vital since large language models excel in lower-order tasks \\nsuch as reproducing information but are limited in their higher order analytical abilities. \\nThese resources explore how to use GenAI to train students in critical analysis and \\ninterrogation.  \\nUse artificial intelligence to get your students thinking critically: Urbi Ghosh \\nof Colorado State University Global shows how GenAI can enhance students’ analytical \\nabilities when used as a critical thinking scaffold. \\nIn an artificially intelligent age, frame higher education around a new kind of \\nthinking: A helpful by-product emerging from the advent of AI is that we are beginning to \\nreflect more critically on the way we think, writes David Holland of the University of East \\nAnglia as he argues for a reimagining of the educational mission. \\nAI detection, cheating and academic integrity \\nThey are questions plaguing many university educators – how can you detect if students \\nhave used artificial intelligence for their work? And does it matter if they have? From the \\ndependability of AI detectors to common features of AI generated content, these \\nresources explore how academics might identify GenAI input and combat cheating but \\nalso whether a new understanding of academic integrity is needed for the digital age. \\nCan academics tell the difference between AI-generated and human-authored \\ncontent? A recent study asked students and academics to distinguish between \\nscientific abstracts generated by ChatGPT and those written by humans. The University \\nof Adelaide's Omar Siddique analyses the results. \\nWill ChatGPT change our definitions of cheating? We can’t yet know if we have a full \\ntaxonomy of ChatGPT-enhanced mischief, or whether certain uses should be classed \\nas mischief at all, writes Tom Muir of Oslo Metropolitan University. \\nCan we detect AI-written content? A look at common features of large language \\nmodel-created writing and its implications for how educators might assess students’ \\nknowledge and skills in the future, by Cesare Giulio Ardito of the University of \\nManchester. \\nIs it time to turn off AI detectors? In this extract from their new book, ‘Teaching with AI: \\nA Practical Guide to a New Era of Human Learning’ , José Antonio Bowen and C. Edward \\nWatson discuss the reliability of AI detection tools and how to combat cheating without \\nthem. \\nHow hard can it be? Testing the dependability of AI detection tools: Students are \\nusing artificial intelligence to write essays and other assessment tasks, but can they\")]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load knowledge base documents\n",
    "loader = DirectoryLoader(DOCUMENT_FOLDER_PATH, glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "docs = loader.load()\n",
    "docs[:2]  # preview first two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70e49e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2024', 'creator': 'Microsoft® Word 2024', 'creationdate': '2025-12-20T19:55:41+08:00', 'author': 'hp404sk7@outlook.com', 'moddate': '2025-12-20T19:55:41+08:00', 'source': 'C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\knowledge_base\\\\AI_and_assessment_in_higher_education.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1'}, page_content='AI and assessment in higher education \\nhttps://www.timeshighereducation.com/campus/ai-and-assessment-higher-\\neducation  \\nThreat or opportunity? Advice for using, managing and embedding artificial intelligence \\nin university assessment, skills development and task design \\nNo sooner had generative AI (GenAI) tools, such as ChatGPT, ignited fears in universities \\nabout risk to assessment practices and academic integrity, than academics began \\nworking out how to embrace it to save time and enrich student skills such as critical \\nthinking and analysis. This has required consideration of not only how to use artificial \\nintelligence (AI) in future university assessment but also a rethink of past exam, \\nassignment and evaluation practices. This diverse collection of resources includes \\nadvice on how to engineer prompts, use AI for authentic assessment design, whether to \\nlean into AI-detection tools, how to build digital literacy and AI’s role in developing soft \\nskills in lifelong learning.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2024', 'creator': 'Microsoft® Word 2024', 'creationdate': '2025-12-20T19:55:41+08:00', 'author': 'hp404sk7@outlook.com', 'moddate': '2025-12-20T19:55:41+08:00', 'source': 'C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\knowledge_base\\\\AI_and_assessment_in_higher_education.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1'}, page_content='lean into AI-detection tools, how to build digital literacy and AI’s role in developing soft \\nskills in lifelong learning. \\nGet started with 25 applications of ChatGPT and generative AI in learning and \\nassessment shared in the form of prompts by Seb Dianati and Suman Laudari of \\nCharles Darwin University. \\nHow AI can affect formative and summative assessment design \\nAs AI technologies become ubiquitous, educators must consider how to design \\nassignments that work with these tools in productive ways to aid learning and AI \\nliteracy. These resources explore practical ways to incorporate AI into task design and \\nassessment strategies, taking into account students’ differing skill levels.  \\nHow students’ GenAI skills and reflection affect assignment instructions: The \\nability to use GenAI is akin to time management or other learning skills that require \\npractice. Here, Vincent Spezzo and Ilya Gokhman from Georgia Tech’s Center for 21st')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "splits[:2]  # show first two splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc7c251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define embeddings model\n",
    "#SiliconFlow hosts open-source embedding models that can be used with LangChain\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"BAAI/bge-m3\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=SILICON_FLOW_BASE_URL,\n",
    "    # Crucial for SiliconFlow/Local providers to avoid dimension errors\n",
    "    check_embedding_ctx_length=False,\n",
    "    chunk_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4c0bb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create vectorstore and retriever\n",
    "VECTOR_DATABASE_PATH = \"C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\chroma_db\"\n",
    "\n",
    "if os.path.exists(VECTOR_DATABASE_PATH):\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=VECTOR_DATABASE_PATH,\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=\"essay_kb\")\n",
    "else:\n",
    "    vectorstore = Chroma.from_documents(\n",
    "            documents=splits, \n",
    "            embedding=embeddings,\n",
    "            collection_name=\"essay_kb\",\n",
    "            persist_directory=VECTOR_DATABASE_PATH\n",
    "        )\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdf87991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define tool used to search knowledge base\n",
    "@tool \n",
    "def search_knowledge_base(query: str) -> str:\n",
    "    \"\"\"Search the internal knowledge base of relevant essay documents.\"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8afc3ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt enforcing the logic\n",
    "system_prompt = \"\"\"You are a fact-checking assistant. \n",
    "Logic Flow:\n",
    "1. For every fact, first use 'search_knowledge_base'.\n",
    "2. If the retrieved info supports the fact, mark as 'correct'.\n",
    "3. If it contradicts, mark as 'wrong'.\n",
    "4. If the knowledge base is insufficient (neutral/unknown), you MUST call 'web_search' ONCE to check online.\n",
    "5. If web results still don't clarify, mark as 'undetermined'.\n",
    "\n",
    "Return the result strictly as a JSON object matching the FactEvaluation schema.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c10dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define output structure\n",
    "class FactEvaluation(BaseModel):\n",
    "    statement: str = Field(description=\"The original fact statement\")\n",
    "    correctness_score: Literal[\"correct\", \"wrong\", \"undetermined\"] = Field(description=\"The final verdict\")\n",
    "    summary_description: str = Field(description=\"Summary of why this verdict was reached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90bef2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RunnableSequence' object has no attribute 'bind_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m#output = asyncio.run(run_fact_checker())\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m output = \u001b[38;5;28;01mawait\u001b[39;00m run_fact_checker()\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fact \u001b[38;5;129;01min\u001b[39;00m output:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28mprint\u001b[39m(fact)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mrun_fact_checker\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     55\u001b[39m inputs = {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [HumanMessage(content=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluate this fact: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)]}\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m#config = {\"configurable\": {\"thread_id\": \"fact_check\"}}\u001b[39;00m\n\u001b[32m     57\u001b[39m \n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Use structured output parsing\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m#response = await agent.ainvoke(inputs)\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# The last message contains the result. \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m final_eval = \u001b[38;5;28;01mawait\u001b[39;00m agent.ainvoke(inputs)\n\u001b[32m     64\u001b[39m results.append(final_eval.model_dump())\n\u001b[32m     65\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValidated fact: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Documents\\repos\\essay-checker-agentic-rag\\venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:3158\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3155\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3156\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3158\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   3159\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3160\u001b[39m     config,\n\u001b[32m   3161\u001b[39m     context=context,\n\u001b[32m   3162\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3163\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3164\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   3165\u001b[39m     print_mode=print_mode,\n\u001b[32m   3166\u001b[39m     output_keys=output_keys,\n\u001b[32m   3167\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   3168\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   3169\u001b[39m     durability=durability,\n\u001b[32m   3170\u001b[39m     **kwargs,\n\u001b[32m   3171\u001b[39m ):\n\u001b[32m   3172\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3173\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Documents\\repos\\essay-checker-agentic-rag\\venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2971\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2969\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2970\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2972\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2973\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2974\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2975\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2976\u001b[39m ):\n\u001b[32m   2977\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2978\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2979\u001b[39m         stream_mode,\n\u001b[32m   2980\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2983\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2984\u001b[39m     ):\n\u001b[32m   2985\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Documents\\repos\\essay-checker-agentic-rag\\venv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:304\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    302\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    305\u001b[39m         t,\n\u001b[32m    306\u001b[39m         retry_policy,\n\u001b[32m    307\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    308\u001b[39m         configurable={\n\u001b[32m    309\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    310\u001b[39m                 _acall,\n\u001b[32m    311\u001b[39m                 weakref.ref(t),\n\u001b[32m    312\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    313\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    314\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    315\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    316\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    317\u001b[39m                 loop=loop,\n\u001b[32m    318\u001b[39m             ),\n\u001b[32m    319\u001b[39m         },\n\u001b[32m    320\u001b[39m     )\n\u001b[32m    321\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Documents\\repos\\essay-checker-agentic-rag\\venv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Documents\\repos\\essay-checker-agentic-rag\\venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:705\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    706\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    707\u001b[39m         )\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    709\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Documents\\repos\\essay-checker-agentic-rag\\venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:473\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Documents\\repos\\essay-checker-agentic-rag\\venv\\Lib\\site-packages\\langchain\\agents\\factory.py:1186\u001b[39m, in \u001b[36mcreate_agent.<locals>.amodel_node\u001b[39m\u001b[34m(state, runtime)\u001b[39m\n\u001b[32m   1173\u001b[39m request = ModelRequest(\n\u001b[32m   1174\u001b[39m     model=model,\n\u001b[32m   1175\u001b[39m     tools=default_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1181\u001b[39m     runtime=runtime,\n\u001b[32m   1182\u001b[39m )\n\u001b[32m   1184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m awrap_model_call_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1185\u001b[39m     \u001b[38;5;66;03m# No async handlers - execute directly\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1186\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m _execute_model_async(request)\n\u001b[32m   1187\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1188\u001b[39m     \u001b[38;5;66;03m# Call composed async handler with base handler\u001b[39;00m\n\u001b[32m   1189\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m awrap_model_call_handler(request, _execute_model_async)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Documents\\repos\\essay-checker-agentic-rag\\venv\\Lib\\site-packages\\langchain\\agents\\factory.py:1152\u001b[39m, in \u001b[36mcreate_agent.<locals>._execute_model_async\u001b[39m\u001b[34m(request)\u001b[39m\n\u001b[32m   1144\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute model asynchronously and return response.\u001b[39;00m\n\u001b[32m   1145\u001b[39m \n\u001b[32m   1146\u001b[39m \u001b[33;03mThis is the core async model execution logic wrapped by `wrap_model_call`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1149\u001b[39m \u001b[33;03mRaises any exceptions that occur during model invocation.\u001b[39;00m\n\u001b[32m   1150\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1151\u001b[39m \u001b[38;5;66;03m# Get the bound model (with auto-detection if needed)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1152\u001b[39m model_, effective_response_format = \u001b[43m_get_bound_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1153\u001b[39m messages = request.messages\n\u001b[32m   1154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request.system_message:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Documents\\repos\\essay-checker-agentic-rag\\venv\\Lib\\site-packages\\langchain\\agents\\factory.py:1083\u001b[39m, in \u001b[36mcreate_agent.<locals>._get_bound_model\u001b[39m\u001b[34m(request)\u001b[39m\n\u001b[32m   1080\u001b[39m \u001b[38;5;66;03m# No structured output - standard model binding\u001b[39;00m\n\u001b[32m   1081\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m final_tools:\n\u001b[32m   1082\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m         \u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind_tools\u001b[49m(\n\u001b[32m   1084\u001b[39m             final_tools, tool_choice=request.tool_choice, **request.model_settings\n\u001b[32m   1085\u001b[39m         ),\n\u001b[32m   1086\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1087\u001b[39m     )\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m request.model.bind(**request.model_settings), \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Documents\\repos\\essay-checker-agentic-rag\\venv\\Lib\\site-packages\\pydantic\\main.py:1026\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m   1023\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1025\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'RunnableSequence' object has no attribute 'bind_tools'",
      "During task with name 'model' and id '7bef7f72-e982-6cf1-e569-3da3441da7d4'"
     ]
    }
   ],
   "source": [
    "#call mcp for web search tool and invoke agent\n",
    "import asyncio\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain.agents import create_agent\n",
    "#import nest_asyncio\n",
    "\n",
    "# This allows async to run inside your notebook's existing event loop\n",
    "#nest_asyncio.apply()\n",
    "\n",
    "JINA_API_KEY = os.getenv(\"JINA_API_KEY\")\n",
    "\n",
    "async def run_fact_checker():\n",
    "    # 2. Setup Client pointing to the REMOTE Jina server\n",
    "    # Note: We filter for 'search' and 'read' tags to save tokens\n",
    "    client = MultiServerMCPClient({\n",
    "        \"jina\": {\n",
    "            \"transport\": \"streamable_http\",\n",
    "            \"url\": \"https://mcp.jina.ai/v1?include_tags=search,read\",\n",
    "            \"headers\": {\n",
    "                \"Authorization\": f\"Bearer {JINA_API_KEY}\"\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "\n",
    "    # 1. Retrieve the web search and read tools from the MCP server\n",
    "    mcp_tools = await client.get_tools()\n",
    "    all_tools = [search_knowledge_base] + mcp_tools\n",
    "\n",
    "    # 2. Initialize your LLM\n",
    "    llm_model = ChatOpenAI(\n",
    "        model=\"deepseek-ai/DeepSeek-V3\",\n",
    "        openai_api_key=OPENAI_API_KEY,\n",
    "        openai_api_base=\"https://api.siliconflow.cn/v1\", \n",
    "    )   \n",
    "\n",
    "    # 3. Create the agent\n",
    "    agent = create_agent(\n",
    "        tools=all_tools,    \n",
    "        system_prompt=system_prompt,\n",
    "        model=llm_model,\n",
    "        name = \"FactCheckerAgent\",\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    with open(FACTS_JSONL_PATH, 'r') as f:\n",
    "        #for line in f:\n",
    "        for i in range(2):\n",
    "            line = f.readline()\n",
    "            fact_data = json.loads(line)\n",
    "            statement = fact_data[\"statement\"]\n",
    "            \n",
    "            # Invoke agent for each fact\n",
    "            inputs = {\"messages\": [HumanMessage(content=f\"Evaluate this fact: {statement}\")]}\n",
    "            #config = {\"configurable\": {\"thread_id\": \"fact_check\"}}\n",
    "            \n",
    "            # Use structured output parsing\n",
    "            response = await agent.ainvoke(inputs)\n",
    "            # The last message contains the result. \n",
    "            # We can force the agent to return the structured schema\n",
    "            structured_llm = llm_model.with_structured_output(FactEvaluation)\n",
    "            final_eval = await structured_llm.ainvoke(response[\"messages\"][-1].content)\n",
    "            \n",
    "            results.append(final_eval.model_dump())\n",
    "            print(f\"Validated fact: {i}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "#output = asyncio.run(run_fact_checker())\n",
    "output = await run_fact_checker()\n",
    "for fact in output:\n",
    "    print(fact)\n",
    "\n",
    "    # 4. Invoke the agent\n",
    "    # Example: Researching the Journal of Intelligent Information Systems (JIIS)\n",
    "    # response = await agent.ainvoke(\n",
    "    #     {\"messages\": [{\"role\": \"user\", \"content\": \"Find the JIIS submission length limits.\"}]}\n",
    "    # )\n",
    "    \n",
    "    # print(response[\"messages\"][-1].content)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d206dbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to C:\\Users\\HP\\Documents\\repos\\essay-checker-agentic-rag\\data\\processed\\fact_checking_output.json\n"
     ]
    }
   ],
   "source": [
    "# export to JSON file\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "Path(FACT_CHECKING_OUTPUT_PATH).write_text(json.dumps(output, indent=2))\n",
    "print(f\"Output saved to {FACT_CHECKING_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4126a62a",
   "metadata": {},
   "source": [
    "# Irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48cf0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define output structure\n",
    "class FactEvaluation(BaseModel):\n",
    "    statement: str = Field(description=\"The original fact statement\")\n",
    "    correctness_score: Literal[\"correct\", \"wrong\", \"undetermined\"] = Field(description=\"The final verdict\")\n",
    "    summary_description: str = Field(description=\"Summary of why this verdict was reached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51082a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt enforcing the logic\n",
    "system_prompt = \"\"\"You are a fact-checking assistant. \n",
    "Logic Flow:\n",
    "1. For every fact, first use 'search_knowledge_base'.\n",
    "2. If the retrieved info supports the fact, mark as 'correct'.\n",
    "3. If it contradicts, mark as 'wrong'.\n",
    "4. If the knowledge base is insufficient (neutral/unknown), you MUST call 'web_search' ONCE to check online.\n",
    "5. If web results still don't clarify, mark as 'undetermined'.\n",
    "\n",
    "Return the result strictly as a JSON object matching the FactEvaluation schema.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37f01d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the tools\n",
    "tools = [search_knowledge_base, web_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88852830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model used\n",
    "llm_model = ChatOpenAI(\n",
    "    model=\"deepseek-ai/DeepSeek-V3\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    openai_api_base=\"https://api.siliconflow.cn/v1\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a336d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the agent\n",
    "agent = create_agent(\n",
    "    tools=tools,    \n",
    "    system_prompt=system_prompt,\n",
    "    model=llm_model,\n",
    "    name = \"FactCheckerAgent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfecddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "    \n",
    "with open(FACTS_JSONL_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        fact_data = json.loads(line)\n",
    "        statement = fact_data[\"statement\"]\n",
    "        \n",
    "        # Invoke agent for each fact\n",
    "        inputs = {\"messages\": [HumanMessage(content=f\"Evaluate this fact: {statement}\")]}\n",
    "        config = {\"configurable\": {\"thread_id\": \"fact_check\"}}\n",
    "        \n",
    "        # Use structured output parsing\n",
    "        response = agent.invoke(inputs)\n",
    "        # The last message contains the result. \n",
    "        # We can force the agent to return the structured schema\n",
    "        structured_llm = llm_model.with_structured_output(FactEvaluation)\n",
    "        final_eval = structured_llm.invoke(response[\"messages\"][-1].content)\n",
    "        \n",
    "        results.append(final_eval.dict())\n",
    "\n",
    "results[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
