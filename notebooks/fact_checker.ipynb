{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafe57d7",
   "metadata": {},
   "source": [
    "### Fact Checker\n",
    "- receives a json of facts, loop through them\n",
    "- for each fact, queries the knowledge base tool to check\n",
    "- if not found, uses web search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fdd3e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install dependencies\n",
    "import requests\n",
    "from langchain.agents import create_agent\n",
    "from langchain.messages import HumanMessage\n",
    "from langchain.tools import tool\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import List, Literal, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21302a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LangChain has its own callback system, and Langfuse listens \n",
    "# to those callbacks to record what your chains and LLMs are doing.\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "langfuse_handler = CallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee108774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Access the variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "SILICON_FLOW_BASE_URL = os.getenv(\"SILICON_FLOW_BASE_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a2dcd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set paths\n",
    "FACTS_JSONL_PATH = \"C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\processed\\\\extracted_facts.jsonl\"\n",
    "ALLOWED_DOMAINS = \"\" # can set to wikipedia domain if want to limit\n",
    "FACT_CHECKING_OUTPUT_PATH = \"C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\processed\\\\fact_checking_output.json\"\n",
    "DOCUMENT_FOLDER_PATH = \"C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\knowledge_base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "675b5186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2024', 'creator': 'Microsoft® Word 2024', 'creationdate': '2025-12-20T19:55:41+08:00', 'author': 'hp404sk7@outlook.com', 'moddate': '2025-12-20T19:55:41+08:00', 'source': 'C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\knowledge_base\\\\AI_and_assessment_in_higher_education.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1'}, page_content='AI and assessment in higher education \\nhttps://www.timeshighereducation.com/campus/ai-and-assessment-higher-\\neducation  \\nThreat or opportunity? Advice for using, managing and embedding artificial intelligence \\nin university assessment, skills development and task design \\nNo sooner had generative AI (GenAI) tools, such as ChatGPT, ignited fears in universities \\nabout risk to assessment practices and academic integrity, than academics began \\nworking out how to embrace it to save time and enrich student skills such as critical \\nthinking and analysis. This has required consideration of not only how to use artificial \\nintelligence (AI) in future university assessment but also a rethink of past exam, \\nassignment and evaluation practices. This diverse collection of resources includes \\nadvice on how to engineer prompts, use AI for authentic assessment design, whether to \\nlean into AI-detection tools, how to build digital literacy and AI’s role in developing soft \\nskills in lifelong learning. \\nGet started with 25 applications of ChatGPT and generative AI in learning and \\nassessment shared in the form of prompts by Seb Dianati and Suman Laudari of \\nCharles Darwin University. \\nHow AI can affect formative and summative assessment design \\nAs AI technologies become ubiquitous, educators must consider how to design \\nassignments that work with these tools in productive ways to aid learning and AI \\nliteracy. These resources explore practical ways to incorporate AI into task design and \\nassessment strategies, taking into account students’ differing skill levels.  \\nHow students’ GenAI skills and reflection affect assignment instructions: The \\nability to use GenAI is akin to time management or other learning skills that require \\npractice. Here, Vincent Spezzo and Ilya Gokhman from Georgia Tech’s Center for 21st \\nCentury Universities offer tips to make sure lecturers’ instructions make sense to \\nstudents with differing levels of AI experience. \\nAI and assessment redesign: a four-step process: If GenAI tools mean institutions \\ncan no longer assure the integrity of individual assessments, the sector must focus on \\nassuring the integrity of overall awards, write Samuel Doherty and Steven Warburton of \\nthe University of Newcastle, Australia. \\nDesigning assessments with generative AI in mind: The proliferation of AI requires a \\nbalance between thoughtfully mitigating and responsibly promoting students’ use of the \\nnew tools. Kate Crane of Dalhousie University offers four strategies to help faculty chart \\na path forward. \\nAI as a learning aid for critical thinking'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2024', 'creator': 'Microsoft® Word 2024', 'creationdate': '2025-12-20T19:55:41+08:00', 'author': 'hp404sk7@outlook.com', 'moddate': '2025-12-20T19:55:41+08:00', 'source': 'C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\knowledge_base\\\\AI_and_assessment_in_higher_education.pdf', 'total_pages': 4, 'page': 1, 'page_label': '2'}, page_content=\"Critical thinking is a future-proof skill in high demand from employers. The arrival of \\nartificial intelligence, specifically GenAI, makes honing critical thinking among students \\nand academics even more vital since large language models excel in lower-order tasks \\nsuch as reproducing information but are limited in their higher order analytical abilities. \\nThese resources explore how to use GenAI to train students in critical analysis and \\ninterrogation.  \\nUse artificial intelligence to get your students thinking critically: Urbi Ghosh \\nof Colorado State University Global shows how GenAI can enhance students’ analytical \\nabilities when used as a critical thinking scaffold. \\nIn an artificially intelligent age, frame higher education around a new kind of \\nthinking: A helpful by-product emerging from the advent of AI is that we are beginning to \\nreflect more critically on the way we think, writes David Holland of the University of East \\nAnglia as he argues for a reimagining of the educational mission. \\nAI detection, cheating and academic integrity \\nThey are questions plaguing many university educators – how can you detect if students \\nhave used artificial intelligence for their work? And does it matter if they have? From the \\ndependability of AI detectors to common features of AI generated content, these \\nresources explore how academics might identify GenAI input and combat cheating but \\nalso whether a new understanding of academic integrity is needed for the digital age. \\nCan academics tell the difference between AI-generated and human-authored \\ncontent? A recent study asked students and academics to distinguish between \\nscientific abstracts generated by ChatGPT and those written by humans. The University \\nof Adelaide's Omar Siddique analyses the results. \\nWill ChatGPT change our definitions of cheating? We can’t yet know if we have a full \\ntaxonomy of ChatGPT-enhanced mischief, or whether certain uses should be classed \\nas mischief at all, writes Tom Muir of Oslo Metropolitan University. \\nCan we detect AI-written content? A look at common features of large language \\nmodel-created writing and its implications for how educators might assess students’ \\nknowledge and skills in the future, by Cesare Giulio Ardito of the University of \\nManchester. \\nIs it time to turn off AI detectors? In this extract from their new book, ‘Teaching with AI: \\nA Practical Guide to a New Era of Human Learning’ , José Antonio Bowen and C. Edward \\nWatson discuss the reliability of AI detection tools and how to combat cheating without \\nthem. \\nHow hard can it be? Testing the dependability of AI detection tools: Students are \\nusing artificial intelligence to write essays and other assessment tasks, but can they\")]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load knowledge base documents\n",
    "loader = DirectoryLoader(DOCUMENT_FOLDER_PATH, glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "docs = loader.load()\n",
    "docs[:2]  # preview first two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70e49e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2024', 'creator': 'Microsoft® Word 2024', 'creationdate': '2025-12-20T19:55:41+08:00', 'author': 'hp404sk7@outlook.com', 'moddate': '2025-12-20T19:55:41+08:00', 'source': 'C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\knowledge_base\\\\AI_and_assessment_in_higher_education.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1'}, page_content='AI and assessment in higher education \\nhttps://www.timeshighereducation.com/campus/ai-and-assessment-higher-\\neducation  \\nThreat or opportunity? Advice for using, managing and embedding artificial intelligence \\nin university assessment, skills development and task design \\nNo sooner had generative AI (GenAI) tools, such as ChatGPT, ignited fears in universities \\nabout risk to assessment practices and academic integrity, than academics began'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2024', 'creator': 'Microsoft® Word 2024', 'creationdate': '2025-12-20T19:55:41+08:00', 'author': 'hp404sk7@outlook.com', 'moddate': '2025-12-20T19:55:41+08:00', 'source': 'C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\knowledge_base\\\\AI_and_assessment_in_higher_education.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1'}, page_content='working out how to embrace it to save time and enrich student skills such as critical \\nthinking and analysis. This has required consideration of not only how to use artificial \\nintelligence (AI) in future university assessment but also a rethink of past exam, \\nassignment and evaluation practices. This diverse collection of resources includes \\nadvice on how to engineer prompts, use AI for authentic assessment design, whether to')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "splits[:2]  # show first two splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc7c251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define embeddings model\n",
    "#SiliconFlow hosts open-source embedding models that can be used with LangChain\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"BAAI/bge-m3\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=SILICON_FLOW_BASE_URL,\n",
    "    # Crucial for SiliconFlow/Local providers to avoid dimension errors\n",
    "    check_embedding_ctx_length=False,\n",
    "    chunk_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4c0bb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create vectorstore and retriever\n",
    "VECTOR_DATABASE_PATH = \"C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\chroma_db\"\n",
    "\n",
    "if os.path.exists(VECTOR_DATABASE_PATH):\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=VECTOR_DATABASE_PATH,\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=\"essay_kb\")\n",
    "else:\n",
    "    vectorstore = Chroma.from_documents(\n",
    "            documents=splits, \n",
    "            embedding=embeddings,\n",
    "            collection_name=\"essay_kb\",\n",
    "            persist_directory=VECTOR_DATABASE_PATH\n",
    "        )\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdf87991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define tool used to search knowledge base\n",
    "@tool \n",
    "def search_knowledge_base(query: str) -> str:\n",
    "    \"\"\"Search the internal knowledge base of relevant essay documents.\"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a9d8eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define output structure\n",
    "class FactEvaluation(BaseModel):\n",
    "    statement: str = Field(description=\"The verbatim excerpt of the statement\")\n",
    "    correctness_score: Literal[\"correct\", \"wrong\", \"undetermined\"] = Field(description=\"The final verdict\")\n",
    "    summary_description: str = Field(description=\"Summary of why this verdict was reached\")\n",
    "    #search_results: str = Field(description=\"A short verbatim of the search results\")\n",
    "    source_document: str = Field(description=\"One source document - if it is from knowledge base, output the *document name* and *page number*, if it is from web, output the *URL*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8afc3ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\n",
    "You are a fact-checking assistant that verifies a single factual statement.\n",
    "\n",
    "You MUST follow the tool usage and decision rules exactly.\n",
    "\n",
    "====================\n",
    "TOOL USAGE RULES\n",
    "====================\n",
    "\n",
    "1. You MUST call `search_knowledge_base` exactly ONCE as your first step.\n",
    "2. After reviewing the retrieved content:\n",
    "   - If the content clearly SUPPORTS the statement → verdict = \"correct\"\n",
    "   - If the content clearly CONTRADICTS the statement → verdict = \"wrong\"\n",
    "   - If the content is unrelated, neutral, ambiguous, or missing key information → verdict is NOT decided yet\n",
    "3. ONLY if the knowledge base is insufficient as defined above, you MAY call `search_web` at most ONCE.\n",
    "4. After web search:\n",
    "   - If web content clearly supports → \"correct\"\n",
    "   - If web content clearly contradicts → \"wrong\"\n",
    "   - If still unclear or conflicting → \"undetermined\"\n",
    "5. You MUST NOT call any tool more than once.\n",
    "\n",
    "====================\n",
    "DEFINITIONS\n",
    "====================\n",
    "\n",
    "- \"Supports\": Explicitly states the same fact without contradiction.\n",
    "- \"Contradicts\": Explicitly states the opposite of the fact.\n",
    "- \"Insufficient\": Mentions the topic but does not confirm or deny the exact claim.\n",
    "\n",
    "====================\n",
    "OUTPUT FORMAT\n",
    "====================\n",
    "\n",
    "Return ONLY a valid JSON object that strictly matches this schema:\n",
    "\n",
    "{FactEvaluation.model_json_schema()}\n",
    "\n",
    "====================\n",
    "SOURCE DOCUMENT RULES\n",
    "====================\n",
    "\n",
    "- If the verdict is based on the knowledge base:\n",
    "  - source_document MUST be: \"<document_name>, page <page_number>\"\n",
    "- If the verdict is based on web search:\n",
    "  - source_document MUST be a single URL\n",
    "- If verdict is \"undetermined\":\n",
    "  - source_document MUST be an empty string \"\"\n",
    "\n",
    "Do NOT include markdown, explanations outside JSON, or multiple sources.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d4483b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Literal, Dict, Any\n",
    "#DONT RUN THIS\n",
    "\n",
    "class EvidenceMetadata(BaseModel):\n",
    "    # Common metadata (shared across sources)\n",
    "    source_type: Literal[\"vectorstore\", \"web\"] = Field(description=\"Where this evidence was retrieved from\")\n",
    "    relevance_score: Optional[float] = Field(description=\"Similarity or relevance score from retriever\", default=None)\n",
    "\n",
    "    # Vectorstore-specific (Chroma + PyPDF)\n",
    "    #document_id: Optional[str] = Field(description=\"Unique ID of the document in Chroma\", default=None)\n",
    "    file_name: Optional[str] = Field(description=\"Original PDF or document filename\", default=None)\n",
    "    page_number: Optional[int] = Field(description=\"Page number in the PDF (if available)\", default=None)\n",
    "    #chunk_id: Optional[str] = Field(description=\"Chunk identifier in vectorstore\", default=None)\n",
    "\n",
    "    # Web-specific (Jina MCP)\n",
    "    url: Optional[str] = Field(description=\"Source URL for web evidence\", default=None)\n",
    "    domain: Optional[str] = Field(description=\"Domain name of the web source\", default=None)\n",
    "    #published_date: Optional[str] = Field(description=\"Publication date if available\", default=None)\n",
    "\n",
    "    # Raw metadata passthrough (future-proof)\n",
    "    raw_metadata: Optional[Dict[str, Any]] = Field(description=\"Unmodified metadata returned by the retriever\", default=None)\n",
    "\n",
    "\n",
    "class Evidence(BaseModel):\n",
    "    excerpt: str = Field(description=\"Verbatim excerpt used as supporting or contradicting evidence\")\n",
    "    metadata: EvidenceMetadata = Field(description=\"Provenance and retrieval details for this evidence\")\n",
    "    supports_statement: Literal[\"supports\", \"contradicts\", \"neutral\"] = Field(description=\"How this evidence relates to the statement\")\n",
    "\n",
    "\n",
    "class FactEvaluation(BaseModel):\n",
    "    statement: str = Field(description=\"The verbatim statement being evaluated\")\n",
    "    correctness_score: Literal[\"correct\", \"wrong\", \"undetermined\"] = Field(description=\"Final verdict after evaluating all evidence\")\n",
    "    summary_description: str = Field(description=\"Concise reasoning explaining how the verdict was reached or not reached\")\n",
    "    evidence: Evidence = Field(description=\"One evidence item used to reach the verdict if it is reached\")\n",
    "    confidence: float = Field(description=\"Confidence score (0-1) for the verdict\", default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d90bef2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"statement\": \"Generative Artificial Intelligence (GenAI) tools, such as ChatGPT and Claude, have fundamentally disrupted the landscape of higher education.\",\n",
      "  \"correctness_score\": \"correct\",\n",
      "  \"summary_description\": \"The knowledge base confirms that GenAI tools like ChatGPT have disrupted higher education, particularly in assessment practices and academic integrity.\",\n",
      "  \"source_document\": \"AI and assessment in higher education, page 1\"\n",
      "}\n",
      "```\n",
      "Validated fact: 0\n",
      "```json\n",
      "{\n",
      "  \"statement\": \"GenAI’s ability to synthesise complex information and generate human-like text presents unprecedented challenges to established educational norms.\",\n",
      "  \"correctness_score\": \"correct\",\n",
      "  \"summary_description\": \"Multiple sources confirm that GenAI's capabilities disrupt traditional educational practices, raising concerns about academic integrity, ethical challenges, and the need for new pedagogical approaches.\",\n",
      "  \"source_document\": \"https://www.mdpi.com/2227-7102/13/9/856\"\n",
      "}\n",
      "```\n",
      "Validated fact: 1\n",
      "{'statement': 'Generative Artificial Intelligence (GenAI) tools, such as ChatGPT and Claude, have fundamentally disrupted the landscape of higher education.', 'correctness_score': 'correct', 'summary_description': 'The knowledge base confirms that GenAI tools like ChatGPT have disrupted higher education, particularly in assessment practices and academic integrity.', 'source_document': 'AI and assessment in higher education, page 1'}\n",
      "{'statement': 'GenAI’s ability to synthesise complex information and generate human-like text presents unprecedented challenges to established educational norms.', 'correctness_score': 'correct', 'summary_description': \"Multiple sources confirm that GenAI's capabilities disrupt traditional educational practices, raising concerns about academic integrity, ethical challenges, and the need for new pedagogical approaches.\", 'source_document': 'https://www.mdpi.com/2227-7102/13/9/856'}\n"
     ]
    }
   ],
   "source": [
    "#call mcp for web search tool and invoke agent\n",
    "import asyncio\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain.agents import create_agent\n",
    "#import nest_asyncio\n",
    "\n",
    "# This allows async to run inside your notebook's existing event loop\n",
    "#nest_asyncio.apply()\n",
    "\n",
    "JINA_API_KEY = os.getenv(\"JINA_API_KEY\")\n",
    "\n",
    "async def run_fact_checker():\n",
    "    # 2. Setup Client pointing to the REMOTE Jina server\n",
    "    # Note: We filter for 'search' and 'read' tags to save tokens\n",
    "    client = MultiServerMCPClient({\n",
    "        \"jina\": {\n",
    "            \"transport\": \"streamable_http\",\n",
    "            \"url\": \"https://mcp.jina.ai/v1?include_tags=search,read\",\n",
    "            \"headers\": {\n",
    "                \"Authorization\": f\"Bearer {JINA_API_KEY}\"\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "\n",
    "    # 1. Retrieve the web search and read tools from the MCP server\n",
    "    mcp_tools = await client.get_tools()\n",
    "    all_tools = [search_knowledge_base] + mcp_tools\n",
    "\n",
    "    # 2. Initialize your LLM\n",
    "    llm_model = ChatOpenAI(\n",
    "        model=\"deepseek-ai/DeepSeek-V3\",\n",
    "        openai_api_key=OPENAI_API_KEY,\n",
    "        openai_api_base=\"https://api.siliconflow.cn/v1\", \n",
    "        temperature=0.2,\n",
    "        max_tokens=800\n",
    "    )   \n",
    "\n",
    "    # 3. Create the agent\n",
    "    agent = create_agent(\n",
    "        tools=all_tools,    \n",
    "        system_prompt=system_prompt,\n",
    "        model=llm_model,\n",
    "        name = \"FactCheckerAgent\",\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    with open(FACTS_JSONL_PATH, 'r') as f:\n",
    "        #for line in f:\n",
    "        for i in range(2):\n",
    "            line = f.readline()\n",
    "            fact_data = json.loads(line)\n",
    "            statement = fact_data[\"statement\"]\n",
    "            \n",
    "            # Invoke agent for each fact\n",
    "            inputs = {\"messages\": [HumanMessage(content=f\"Evaluate this fact: {statement}\")]}\n",
    "            #config = {\"configurable\": {\"thread_id\": \"fact_check\"}}\n",
    "            \n",
    "            # Use structured output parsing\n",
    "            response = await agent.ainvoke(inputs, config={\"callbacks\":[langfuse_handler]})\n",
    "            # The last message contains the result. \n",
    "            print(response[\"messages\"][-1].content)\n",
    "            # We can force the agent to return the structured schema\n",
    "            structured_llm = llm_model.with_structured_output(FactEvaluation)\n",
    "            final_eval = await structured_llm.ainvoke(response[\"messages\"][-1].content, config={\"callbacks\":[langfuse_handler]})\n",
    "            \n",
    "            results.append(final_eval.model_dump())\n",
    "            print(f\"Validated fact: {i}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "#output = asyncio.run(run_fact_checker())\n",
    "output = await run_fact_checker()\n",
    "for fact in output:\n",
    "    print(fact)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d206dbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to C:\\Users\\HP\\Documents\\repos\\essay-checker-agentic-rag\\data\\processed\\fact_checking_output.json\n"
     ]
    }
   ],
   "source": [
    "# export to JSON file\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "Path(FACT_CHECKING_OUTPUT_PATH).write_text(json.dumps(output, indent=2))\n",
    "print(f\"Output saved to {FACT_CHECKING_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4126a62a",
   "metadata": {},
   "source": [
    "# Irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48cf0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define output structure\n",
    "class FactEvaluation(BaseModel):\n",
    "    statement: str = Field(description=\"The original fact statement\")\n",
    "    correctness_score: Literal[\"correct\", \"wrong\", \"undetermined\"] = Field(description=\"The final verdict\")\n",
    "    summary_description: str = Field(description=\"Summary of why this verdict was reached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51082a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt enforcing the logic\n",
    "system_prompt = \"\"\"You are a fact-checking assistant. \n",
    "Logic Flow:\n",
    "1. For every fact, first use 'search_knowledge_base'.\n",
    "2. If the retrieved info supports the fact, mark as 'correct'.\n",
    "3. If it contradicts, mark as 'wrong'.\n",
    "4. If the knowledge base is insufficient (neutral/unknown), you MUST call 'web_search' ONCE to check online.\n",
    "5. If web results still don't clarify, mark as 'undetermined'.\n",
    "\n",
    "Return the result strictly as a JSON object matching the FactEvaluation schema.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37f01d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the tools\n",
    "tools = [search_knowledge_base, web_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88852830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model used\n",
    "llm_model = ChatOpenAI(\n",
    "    model=\"deepseek-ai/DeepSeek-V3\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    openai_api_base=\"https://api.siliconflow.cn/v1\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a336d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the agent\n",
    "agent = create_agent(\n",
    "    tools=tools,    \n",
    "    system_prompt=system_prompt,\n",
    "    model=llm_model,\n",
    "    name = \"FactCheckerAgent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfecddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "    \n",
    "with open(FACTS_JSONL_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        fact_data = json.loads(line)\n",
    "        statement = fact_data[\"statement\"]\n",
    "        \n",
    "        # Invoke agent for each fact\n",
    "        inputs = {\"messages\": [HumanMessage(content=f\"Evaluate this fact: {statement}\")]}\n",
    "        config = {\"configurable\": {\"thread_id\": \"fact_check\"}}\n",
    "        \n",
    "        # Use structured output parsing\n",
    "        response = agent.invoke(inputs)\n",
    "        # The last message contains the result. \n",
    "        # We can force the agent to return the structured schema\n",
    "        structured_llm = llm_model.with_structured_output(FactEvaluation)\n",
    "        final_eval = structured_llm.invoke(response[\"messages\"][-1].content)\n",
    "        \n",
    "        results.append(final_eval.dict())\n",
    "\n",
    "results[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
