{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e3ea95",
   "metadata": {},
   "source": [
    "## Language Checker Pipeline\n",
    "- checks the quality of the essay from a linguistics perspective\n",
    "- including grammar vocabulary, structure etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09ab8503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a571b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d45cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2024', 'creator': 'Microsoft® Word 2024', 'creationdate': '2025-12-16T14:12:31+08:00', 'author': 'hp404sk7@outlook.com', 'moddate': '2025-12-16T14:12:31+08:00', 'source': 'C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\raw\\\\essay_content.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}, page_content='2. Student Essay Submission \\nTitle \\nBeyond Prohibition: Integrating Generative AI into Higher Education Assessment \\nand Learning \\n \\nIntroduction \\nThe rapid emergence of Generative Artificial Intelligence (GenAI) tools, such as ChatGPT \\nand Claude, has fundamentally disrupted the landscape of higher education. While \\ndigital tools have long supported academic study, GenAI’s ability to synthesise complex \\ninformation and generate human-like text presents unprecedented challenges to \\nestablished educational norms. \\nA key concern among educators is the threat these tools pose to academic integrity, \\nparticularly in relation to traditional assessment methods. However, an exclusive focus \\non these risks overlooks the significant potential of GenAI to enhance personalised \\nlearning. \\nThis essay argues that although GenAI undermines the reliability of conventional essay-\\nbased assessments, it also offers meaningful benefits for student engagement and skill \\ndevelopment. As a result, universities should move beyond outright prohibition and \\nadopt a policy of critical integration by redesigning assessments to coexist with \\nemerging technologies. The discussion will first explore challenges to academic \\nintegrity, then examine learning benefits, and finally address the need for assessment \\nreform. \\n \\nBody Paragraph 1: Challenges to Academic Integrity \\nThe most immediate challenge posed by GenAI is the erosion of academic integrity in \\ntraditional written assessments. Coursework that relies heavily on knowledge \\nsummarisation or standard essay writing is especially vulnerable to AI-generated \\nplagiarism. Baron (2023) highlights that existing plagiarism detection software \\nincreasingly struggles to distinguish between human and AI-produced text, creating a \\n“crisis of trust” in grading systems. \\nThis issue stems from the way GenAI generates original outputs by predicting word \\nsequences rather than copying text verbatim, allowing it to evade similarity-based \\ndetection tools (Zhang & Li, 2024). As a result, students can submit high-quality work \\nwith limited cognitive engagement, undermining the credibility of academic \\nqualifications.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2024', 'creator': 'Microsoft® Word 2024', 'creationdate': '2025-12-16T14:12:31+08:00', 'author': 'hp404sk7@outlook.com', 'moddate': '2025-12-16T14:12:31+08:00', 'source': 'C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\raw\\\\essay_content.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}, page_content='Although some institutions have implemented strict bans, enforcement is largely \\nineffective outside controlled examination settings. Consequently, continued reliance \\non take-home essays as a primary measure of learning is becoming increasingly \\nunsustainable. \\n \\nBody Paragraph 2: Benefits for Student Learning \\nDespite integrity concerns, GenAI offers significant opportunities to enhance learning \\nthrough personalisation and instant feedback. Unlike human tutors, AI tools can provide \\ncontinuous academic support without time constraints. Chen et al. (2024) demonstrate \\nthat when AI is used as a Socratic tutor—prompting learners through guided questioning \\nrather than direct answers—students show improved critical thinking skills, particularly \\nin STEM disciplines. \\nThis indicates that GenAI can effectively scaffold learning by adapting explanations to \\nindividual proficiency levels. Additionally, language and formatting support offered by AI \\nallows students, especially second-language writers, to focus on higher-order thinking \\nrather than surface-level accuracy (EduTech Future, 2023). \\nTherefore, rather than replacing learning, GenAI can function as an assistive tool that \\nbroadens access to academic support and enhances learner autonomy. \\n \\nBody Paragraph 3: The Need for Assessment Reform \\nGiven both the risks and benefits of GenAI, assessment reform represents the most \\nsustainable response. Universities must shift from evaluating what students know to \\nassessing how they apply knowledge in conjunction with technology. A prohibition-\\nfocused approach fails to acknowledge that AI literacy is increasingly expected in \\nprofessional contexts (World Economic Forum, 2024). \\nAssessment methods should evolve to include oral examinations, reflective portfolios, \\nand tasks that require critical evaluation of AI-generated outputs. The University of \\nManchester Teaching Framework (2023) emphasises the importance of prioritising \\nlearning processes over final products, encouraging students to demonstrate inquiry, \\nverification, and reflection skills. \\nBy embedding AI use within assessment design—such as asking students to critique \\nflawed AI responses—institutions can cultivate critical digital literacy that aligns with \\ncontemporary academic and workplace demands. \\n \\nConclusion'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2024', 'creator': 'Microsoft® Word 2024', 'creationdate': '2025-12-16T14:12:31+08:00', 'author': 'hp404sk7@outlook.com', 'moddate': '2025-12-16T14:12:31+08:00', 'source': 'C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\raw\\\\essay_content.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}, page_content='In conclusion, the integration of GenAI in higher education presents a clear paradox: it \\nthreatens the integrity of traditional assessments while simultaneously offering \\npowerful tools for personalised learning. As this essay has demonstrated, banning AI \\ntechnologies is an impractical and short-sighted response that leaves students \\nunprepared for a technology-driven future. \\nInstead, universities should adopt a balanced approach that integrates AI literacy into \\ncurricula while redesigning assessments to prioritise critical thinking, authenticity, and \\nreflective practice. Ultimately, the rise of GenAI does not mark the decline of higher \\neducation but signals an essential evolution towards more resilient and relevant \\npedagogical models.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the documents\n",
    "pdf_path = \"C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\raw\\\\essay_content.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "if not docs:\n",
    "        print(\"Error: No documents found.\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74e531d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the language check schema (output format)\n",
    "# We need specific linguistic data points for the final grader.\n",
    "\n",
    "class GrammarError(BaseModel):\n",
    "    original_text: str = Field(..., description=\"The exact text snippet containing the error.\")\n",
    "    correction: str = Field(..., description=\"The corrected version of the text.\")\n",
    "    error_type: str = Field(..., description=\"Type of error (e.g., 'Subject-Verb Agreement', 'Punctuation', 'Spelling').\")\n",
    "    explanation: str = Field(..., description=\"Brief explanation of why this is an error.\")\n",
    "\n",
    "class VocabularyAnalysis(BaseModel):\n",
    "    score: int = Field(..., description=\"A score from 1-10 rating the vocabulary sophistication.\")\n",
    "    repetitive_words: List[str] = Field(..., description=\"List of words used excessively (excluding common stop words).\")\n",
    "    advanced_words_used: List[str] = Field(..., description=\"List of sophisticated or domain-specific words used correctly.\")\n",
    "    feedback: str = Field(..., description=\"Qualitative feedback on word choice and variety.\")\n",
    "\n",
    "class StructureAnalysis(BaseModel):\n",
    "    sentence_variety_score: int = Field(..., description=\"A score from 1-10 on sentence length and structure variety.\")\n",
    "    flow_issues: List[str] = Field(..., description=\"List of specific issues with flow, transitions, or paragraph cohesion.\")\n",
    "    feedback: str = Field(..., description=\"Qualitative feedback on the overall structure and flow.\")\n",
    "\n",
    "class LanguageAnalysisResult(BaseModel):\n",
    "    grammar_issues: List[GrammarError] = Field(default_factory=list, description=\"List of specific grammar and mechanics errors found.\")\n",
    "    vocabulary: VocabularyAnalysis = Field(..., description=\"Analysis of the student's vocabulary usage.\")\n",
    "    structure: StructureAnalysis = Field(..., description=\"Analysis of sentence structure and essay flow.\")\n",
    "    overall_tone: str = Field(..., description=\"Description of the essay's tone (e.g., 'Formal', 'Casual', 'Inconsistent').\")\n",
    "    summary_critique: str = Field(..., description=\"A concise summary of the linguistic quality for the final grader.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91b074de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the Model (DeepSeek via OpenAI API) ---\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-ai/DeepSeek-V3\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    openai_api_base=\"https://api.siliconflow.cn/v1\",\n",
    "    temperature=0  # Keep it 0 for consistent analysis\n",
    ")\n",
    "\n",
    "# Bind the robust schema\n",
    "structured_llm = llm.with_structured_output(LanguageAnalysisResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1039a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Prompt ---\n",
    "system_prompt = \"\"\"\n",
    "You are a strict Linguistic Professor and Editor. \n",
    "Your goal is to analyze the student's essay purely on **language mechanics, style, and structure**. \n",
    "Do NOT grade the content or arguments; focus only on HOW it is written.\n",
    "\n",
    "Analyze the text for:\n",
    "1. Grammar, punctuation, and spelling errors (be specific).\n",
    "2. Vocabulary sophistication and redundancy.\n",
    "3. Sentence structure variety (simple vs. complex) and flow.\n",
    "4. Tone consistency.\n",
    "\n",
    "Provide a detailed structured output that a final grader can use to penalize or reward the student.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc6e5db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"Here is the student's essay:\\n\\n{text}\")\n",
    "])\n",
    "\n",
    "chain = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31c50470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 3 pages (5227 characters)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_4436\\2905930422.py:7: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  result_json = result.dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'grammar_issues': [{'original_text': 'synthesise',\n",
       "   'correction': 'synthesize',\n",
       "   'error_type': 'Spelling (British vs. American English)',\n",
       "   'explanation': \"'Synthesise' is British spelling; consistency should be maintained if American spelling is used elsewhere.\"},\n",
       "  {'original_text': 'standard essay writing',\n",
       "   'correction': 'standard essay-writing',\n",
       "   'error_type': 'Hyphenation',\n",
       "   'explanation': 'Compound adjective before a noun requires hyphenation.'},\n",
       "  {'original_text': 'Chen et al. (2024) demonstrate',\n",
       "   'correction': 'Chen et al. (2024) demonstrate',\n",
       "   'error_type': 'Subject-verb agreement',\n",
       "   'explanation': \"'Chen et al.' is plural (referring to multiple authors), so 'demonstrate' is correct.\"}],\n",
       " 'vocabulary': {'score': 8,\n",
       "  'repetitive_words': [],\n",
       "  'advanced_words_used': ['unprecedented',\n",
       "   'Socratic',\n",
       "   'scaffold',\n",
       "   'pedagogical'],\n",
       "  'feedback': 'Precise and discipline-appropriate. No unnecessary repetition.'},\n",
       " 'structure': {'sentence_variety_score': 4,\n",
       "  'flow_issues': [\"Minor abruptness transitioning from 'Body Paragraph 1' to 'Body Paragraph 2'. Consider adding a bridging sentence.\",\n",
       "   'Conclusion effectively synthesizes arguments without introducing new ideas.'],\n",
       "  'feedback': 'Clear progression from challenges to benefits to solutions. Topic sentences effectively signpost content.'},\n",
       " 'overall_tone': 'Academic (appropriate)',\n",
       " 'summary_critique': \"The essay demonstrates strong academic tone and logical structure. Vocabulary is sophisticated ('unprecedented', 'scaffold', 'pedagogical') with minimal redundancy. Sentence structures vary appropriately between complex and simple constructions. Minor grammatical inconsistencies (hyphenation, spelling variants) and occasional abrupt transitions slightly disrupt flow.\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute \n",
    "full_text = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "print(f\"Analyzing {len(docs)} pages ({len(full_text)} characters)...\")\n",
    "try:\n",
    "    # Invoke the chain with the full text\n",
    "    result = chain.invoke({\"text\": full_text})\n",
    "    result_json = result.dict()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during analysis: {e}\")\n",
    "\n",
    "result_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01526995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to C:\\Users\\HP\\Documents\\repos\\essay-checker-agentic-rag\\data\\processed\\language_analysis_output.json\n"
     ]
    }
   ],
   "source": [
    "# export to JSON file\n",
    "LANGUAGE_ANALYSIS_OUTPUT_PATH = \"C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\processed\\\\language_analysis_output.json\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "Path(LANGUAGE_ANALYSIS_OUTPUT_PATH).write_text(json.dumps(result_json, indent=2))\n",
    "print(f\"Output saved to {LANGUAGE_ANALYSIS_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bda7e2",
   "metadata": {},
   "source": [
    "### Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3b734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install dependencies (if not already installed)\n",
    "# !pip install langchain langchain-openai langchain-community pydantic pypdf python-dotenv\n",
    "\n",
    "import os\n",
    "from typing import List, Optional\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# --- Step 1: Define the \"Language Check\" Schema ---\n",
    "# We need specific linguistic data points for the final grader.\n",
    "\n",
    "class GrammarError(BaseModel):\n",
    "    original_text: str = Field(..., description=\"The exact text snippet containing the error.\")\n",
    "    correction: str = Field(..., description=\"The corrected version of the text.\")\n",
    "    error_type: str = Field(..., description=\"Type of error (e.g., 'Subject-Verb Agreement', 'Punctuation', 'Spelling').\")\n",
    "    explanation: str = Field(..., description=\"Brief explanation of why this is an error.\")\n",
    "\n",
    "class VocabularyAnalysis(BaseModel):\n",
    "    score: int = Field(..., description=\"A score from 1-10 rating the vocabulary sophistication.\")\n",
    "    repetitive_words: List[str] = Field(..., description=\"List of words used excessively (excluding common stop words).\")\n",
    "    advanced_words_used: List[str] = Field(..., description=\"List of sophisticated or domain-specific words used correctly.\")\n",
    "    feedback: str = Field(..., description=\"Qualitative feedback on word choice and variety.\")\n",
    "\n",
    "class StructureAnalysis(BaseModel):\n",
    "    sentence_variety_score: int = Field(..., description=\"A score from 1-10 on sentence length and structure variety.\")\n",
    "    flow_issues: List[str] = Field(..., description=\"List of specific issues with flow, transitions, or paragraph cohesion.\")\n",
    "    feedback: str = Field(..., description=\"Qualitative feedback on the overall structure and flow.\")\n",
    "\n",
    "class LanguageAnalysisResult(BaseModel):\n",
    "    grammar_issues: List[GrammarError] = Field(default_factory=list, description=\"List of specific grammar and mechanics errors found.\")\n",
    "    vocabulary: VocabularyAnalysis = Field(..., description=\"Analysis of the student's vocabulary usage.\")\n",
    "    structure: StructureAnalysis = Field(..., description=\"Analysis of sentence structure and essay flow.\")\n",
    "    overall_tone: str = Field(..., description=\"Description of the essay's tone (e.g., 'Formal', 'Casual', 'Inconsistent').\")\n",
    "    summary_critique: str = Field(..., description=\"A concise summary of the linguistic quality for the final grader.\")\n",
    "\n",
    "# --- Step 2: Setup the Model (DeepSeek via OpenAI API) ---\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-ai/DeepSeek-V3\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    openai_api_base=\"https://api.siliconflow.cn/v1\",\n",
    "    temperature=0  # Keep it 0 for consistent analysis\n",
    ")\n",
    "\n",
    "# Bind the robust schema\n",
    "structured_llm = llm.with_structured_output(LanguageAnalysisResult)\n",
    "\n",
    "# --- Step 3: Define the Prompt ---\n",
    "system_prompt = \"\"\"\n",
    "You are a strict Linguistic Professor and Editor. \n",
    "Your goal is to analyze the student's essay purely on **language mechanics, style, and structure**. \n",
    "Do NOT grade the content or arguments; focus only on HOW it is written.\n",
    "\n",
    "Analyze the text for:\n",
    "1. Grammar, punctuation, and spelling errors (be specific).\n",
    "2. Vocabulary sophistication and redundancy.\n",
    "3. Sentence structure variety (simple vs. complex) and flow.\n",
    "4. Tone consistency.\n",
    "\n",
    "Provide a detailed structured output that a final grader can use to penalize or reward the student.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"Here is the student's essay:\\n\\n{text}\")\n",
    "])\n",
    "\n",
    "chain = prompt | structured_llm\n",
    "\n",
    "# --- Step 4: Robust Execution Pipeline ---\n",
    "\n",
    "def run_language_check(pdf_path: str):\n",
    "    print(f\"Loading PDF from: {pdf_path}\")\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    if not docs:\n",
    "        print(\"Error: No documents found.\")\n",
    "        return None\n",
    "\n",
    "    # CRITICAL CHANGE: \n",
    "    # For language checking (flow, repetition, consistency), the model needs \n",
    "    # the FULL context. DeepSeek has a large context window, so we merge pages.\n",
    "    full_text = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "    \n",
    "    print(f\"Analyzing {len(docs)} pages ({len(full_text)} characters)...\")\n",
    "    \n",
    "    try:\n",
    "        # Invoke the chain with the full text\n",
    "        result = chain.invoke({\"text\": full_text})\n",
    "        \n",
    "        # Output is already a Pydantic object (LanguageAnalysisResult)\n",
    "        return result.dict()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Usage ---\n",
    "pdf_path = r\"C:\\Users\\HP\\Documents\\repos\\essay-checker-agentic-rag\\data\\raw\\essay_content.pdf\"\n",
    "result_json = run_language_check(pdf_path)\n",
    "\n",
    "if result_json:\n",
    "    import json\n",
    "    # Print pretty JSON\n",
    "    print(json.dumps(result_json, indent=2))\n",
    "else:\n",
    "    print(\"Analysis failed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
