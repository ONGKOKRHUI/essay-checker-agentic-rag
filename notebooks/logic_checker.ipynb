{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e3ea95",
   "metadata": {},
   "source": [
    "## Logic Checker Pipeline\n",
    "- checks the essay from a logical perspective\n",
    "- including the relevance of the topic etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09ab8503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a571b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d45cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2024', 'creator': 'Microsoft® Word 2024', 'creationdate': '2025-12-16T14:12:31+08:00', 'author': 'hp404sk7@outlook.com', 'moddate': '2025-12-16T14:12:31+08:00', 'source': 'C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\raw\\\\essay_content.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}, page_content='2. Student Essay Submission \\nTitle \\nBeyond Prohibition: Integrating Generative AI into Higher Education Assessment \\nand Learning \\n \\nIntroduction \\nThe rapid emergence of Generative Artificial Intelligence (GenAI) tools, such as ChatGPT \\nand Claude, has fundamentally disrupted the landscape of higher education. While \\ndigital tools have long supported academic study, GenAI’s ability to synthesise complex \\ninformation and generate human-like text presents unprecedented challenges to \\nestablished educational norms. \\nA key concern among educators is the threat these tools pose to academic integrity, \\nparticularly in relation to traditional assessment methods. However, an exclusive focus \\non these risks overlooks the significant potential of GenAI to enhance personalised \\nlearning. \\nThis essay argues that although GenAI undermines the reliability of conventional essay-\\nbased assessments, it also offers meaningful benefits for student engagement and skill \\ndevelopment. As a result, universities should move beyond outright prohibition and \\nadopt a policy of critical integration by redesigning assessments to coexist with \\nemerging technologies. The discussion will first explore challenges to academic \\nintegrity, then examine learning benefits, and finally address the need for assessment \\nreform. \\n \\nBody Paragraph 1: Challenges to Academic Integrity \\nThe most immediate challenge posed by GenAI is the erosion of academic integrity in \\ntraditional written assessments. Coursework that relies heavily on knowledge \\nsummarisation or standard essay writing is especially vulnerable to AI-generated \\nplagiarism. Baron (2023) highlights that existing plagiarism detection software \\nincreasingly struggles to distinguish between human and AI-produced text, creating a \\n“crisis of trust” in grading systems. \\nThis issue stems from the way GenAI generates original outputs by predicting word \\nsequences rather than copying text verbatim, allowing it to evade similarity-based \\ndetection tools (Zhang & Li, 2024). As a result, students can submit high-quality work \\nwith limited cognitive engagement, undermining the credibility of academic \\nqualifications.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2024', 'creator': 'Microsoft® Word 2024', 'creationdate': '2025-12-16T14:12:31+08:00', 'author': 'hp404sk7@outlook.com', 'moddate': '2025-12-16T14:12:31+08:00', 'source': 'C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\raw\\\\essay_content.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}, page_content='Although some institutions have implemented strict bans, enforcement is largely \\nineffective outside controlled examination settings. Consequently, continued reliance \\non take-home essays as a primary measure of learning is becoming increasingly \\nunsustainable. \\n \\nBody Paragraph 2: Benefits for Student Learning \\nDespite integrity concerns, GenAI offers significant opportunities to enhance learning \\nthrough personalisation and instant feedback. Unlike human tutors, AI tools can provide \\ncontinuous academic support without time constraints. Chen et al. (2024) demonstrate \\nthat when AI is used as a Socratic tutor—prompting learners through guided questioning \\nrather than direct answers—students show improved critical thinking skills, particularly \\nin STEM disciplines. \\nThis indicates that GenAI can effectively scaffold learning by adapting explanations to \\nindividual proficiency levels. Additionally, language and formatting support offered by AI \\nallows students, especially second-language writers, to focus on higher-order thinking \\nrather than surface-level accuracy (EduTech Future, 2023). \\nTherefore, rather than replacing learning, GenAI can function as an assistive tool that \\nbroadens access to academic support and enhances learner autonomy. \\n \\nBody Paragraph 3: The Need for Assessment Reform \\nGiven both the risks and benefits of GenAI, assessment reform represents the most \\nsustainable response. Universities must shift from evaluating what students know to \\nassessing how they apply knowledge in conjunction with technology. A prohibition-\\nfocused approach fails to acknowledge that AI literacy is increasingly expected in \\nprofessional contexts (World Economic Forum, 2024). \\nAssessment methods should evolve to include oral examinations, reflective portfolios, \\nand tasks that require critical evaluation of AI-generated outputs. The University of \\nManchester Teaching Framework (2023) emphasises the importance of prioritising \\nlearning processes over final products, encouraging students to demonstrate inquiry, \\nverification, and reflection skills. \\nBy embedding AI use within assessment design—such as asking students to critique \\nflawed AI responses—institutions can cultivate critical digital literacy that aligns with \\ncontemporary academic and workplace demands. \\n \\nConclusion'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2024', 'creator': 'Microsoft® Word 2024', 'creationdate': '2025-12-16T14:12:31+08:00', 'author': 'hp404sk7@outlook.com', 'moddate': '2025-12-16T14:12:31+08:00', 'source': 'C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\raw\\\\essay_content.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}, page_content='In conclusion, the integration of GenAI in higher education presents a clear paradox: it \\nthreatens the integrity of traditional assessments while simultaneously offering \\npowerful tools for personalised learning. As this essay has demonstrated, banning AI \\ntechnologies is an impractical and short-sighted response that leaves students \\nunprepared for a technology-driven future. \\nInstead, universities should adopt a balanced approach that integrates AI literacy into \\ncurricula while redesigning assessments to prioritise critical thinking, authenticity, and \\nreflective practice. Ultimately, the rise of GenAI does not mark the decline of higher \\neducation but signals an essential evolution towards more resilient and relevant \\npedagogical models.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the documents\n",
    "pdf_path = \"C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\raw\\\\essay_content.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "if not docs:\n",
    "        print(\"Error: No documents found.\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e531d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the \"Logic & Relevance\" Schema ---\n",
    "\n",
    "class LogicalFallacy(BaseModel):\n",
    "    fallacy_type: str = Field(..., description=\"Name of the fallacy (e.g., 'Circular Reasoning', 'Straw Man', 'Hasty Generalization').\")\n",
    "    location_snippet: str = Field(..., description=\"The quote from the text containing the fallacy.\")\n",
    "    explanation: str = Field(..., description=\"Why this argument is logically flawed.\")\n",
    "\n",
    "class RelevanceAnalysis(BaseModel):\n",
    "    is_off_topic: bool = Field(..., description=\"True if the essay completely fails to address the prompt.\")\n",
    "    score: int = Field(..., description=\"Score 1-10. How directly does it answer the specific question asked?\")\n",
    "    thesis_alignment: str = Field(..., description=\"Analysis of whether the thesis statement directly addresses the prompt.\")\n",
    "    missing_key_points: List[str] = Field(..., description=\"List of key concepts related to the question that the student failed to mention.\")\n",
    "\n",
    "class StructureAnalysis(BaseModel):\n",
    "    has_clear_intro: bool = Field(..., description=\"Does it have a distinct introduction?\")\n",
    "    has_clear_conclusion: bool = Field(..., description=\"Does it have a distinct conclusion?\")\n",
    "    flow_score: int = Field(..., description=\"Score 1-10. How well do paragraphs transition and build upon each other?\")\n",
    "    structural_weaknesses: List[str] = Field(..., description=\"List of specific structural issues (e.g., 'Sudden topic change in Para 3').\")\n",
    "\n",
    "class LogicAnalysisResult(BaseModel):\n",
    "    relevance: RelevanceAnalysis = Field(..., description=\"Analysis of how well the essay answers the prompt.\")\n",
    "    structure: StructureAnalysis = Field(..., description=\"Analysis of the essay's organization.\")\n",
    "    identified_fallacies: List[LogicalFallacy] = Field(default_factory=list, description=\"List of logical errors found in the argumentation.\")\n",
    "    argument_strength_score: int = Field(..., description=\"Score 1-10 on the overall persuasiveness and soundness of arguments.\")\n",
    "    summary_critique: str = Field(..., description=\"A concise summary of the logical quality for the final grader.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91b074de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the Model (DeepSeek via OpenAI API) ---\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-ai/DeepSeek-V3\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    openai_api_base=\"https://api.siliconflow.cn/v1\",\n",
    "    temperature=0  # Keep it 0 for consistent analysis\n",
    ")\n",
    "\n",
    "# Bind the robust schema\n",
    "structured_llm = llm.with_structured_output(LogicAnalysisResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Prompt ---\n",
    "system_prompt = \"\"\"\n",
    "You are a strict Essay Editor and Logic Expert.\n",
    "Your task is to ruthlessly evaluate the **Relevance** and **Logic** of the student's essay against the provided Question.\n",
    "\n",
    "**Analysis Directives:**\n",
    "1. **Relevance is Paramount:** If the essay is well-written but answers the wrong question, it must receive a low Relevance Score and `is_off_topic=True`.\n",
    "2. **Logical Rigor:** Hunt for logical fallacies. Does the conclusion follow from the premises? Are the claims supported by evidence or just asserted?\n",
    "3. **Structure:** Check for standard academic form (Intro -> Body Arguments -> Conclusion).\n",
    "\n",
    "Output strictly in the requested JSON format.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc6e5db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"Here is the student's essay:\\n\\n{text}\")\n",
    "])\n",
    "\n",
    "chain = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c50470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 3 pages (5227 characters)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_27804\\2905930422.py:7: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  result_json = result.dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'relevance': {'is_off_topic': False,\n",
       "  'score': 10,\n",
       "  'thesis_alignment': 'The essay effectively addresses the question of integrating Generative AI into higher education assessment and learning, maintaining strong relevance throughout.',\n",
       "  'missing_key_points': []},\n",
       " 'structure': {'has_clear_intro': True,\n",
       "  'has_clear_conclusion': True,\n",
       "  'flow_score': 9,\n",
       "  'structural_weaknesses': []},\n",
       " 'identified_fallacies': [],\n",
       " 'argument_strength_score': 9,\n",
       " 'summary_critique': 'The essay presents a well-structured and logically rigorous argument for integrating Generative AI into higher education. It effectively balances the discussion between challenges to academic integrity and the benefits for student learning, culminating in a persuasive call for assessment reform. The argument is supported by relevant evidence and maintains strong relevance to the topic throughout.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute \n",
    "full_text = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "print(f\"Analyzing {len(docs)} pages ({len(full_text)} characters)...\")\n",
    "try:\n",
    "    # Invoke the chain with the full text\n",
    "    result = chain.invoke({\"text\": full_text})\n",
    "    result_json = result.dict()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during analysis: {e}\")\n",
    "\n",
    "result_json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01526995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to C:\\Users\\HP\\Documents\\repos\\essay-checker-agentic-rag\\data\\processed\\logic_analysis_output.json\n"
     ]
    }
   ],
   "source": [
    "# export to JSON file\n",
    "LOGIC_ANALYSIS_OUTPUT_PATH = \"C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\processed\\\\logic_analysis_output.json\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "Path(LOGIC_ANALYSIS_OUTPUT_PATH).write_text(json.dumps(result_json, indent=2))\n",
    "print(f\"Output saved to {LOGIC_ANALYSIS_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bda7e2",
   "metadata": {},
   "source": [
    "### Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3b734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# --- Step 1: Define the \"Logic & Relevance\" Schema ---\n",
    "\n",
    "class LogicalFallacy(BaseModel):\n",
    "    fallacy_type: str = Field(..., description=\"Name of the fallacy (e.g., 'Circular Reasoning', 'Straw Man', 'Hasty Generalization').\")\n",
    "    location_snippet: str = Field(..., description=\"The quote from the text containing the fallacy.\")\n",
    "    explanation: str = Field(..., description=\"Why this argument is logically flawed.\")\n",
    "\n",
    "class RelevanceAnalysis(BaseModel):\n",
    "    is_off_topic: bool = Field(..., description=\"True if the essay completely fails to address the prompt.\")\n",
    "    score: int = Field(..., description=\"Score 1-10. How directly does it answer the specific question asked?\")\n",
    "    thesis_alignment: str = Field(..., description=\"Analysis of whether the thesis statement directly addresses the prompt.\")\n",
    "    missing_key_points: List[str] = Field(..., description=\"List of key concepts related to the question that the student failed to mention.\")\n",
    "\n",
    "class StructureAnalysis(BaseModel):\n",
    "    has_clear_intro: bool = Field(..., description=\"Does it have a distinct introduction?\")\n",
    "    has_clear_conclusion: bool = Field(..., description=\"Does it have a distinct conclusion?\")\n",
    "    flow_score: int = Field(..., description=\"Score 1-10. How well do paragraphs transition and build upon each other?\")\n",
    "    structural_weaknesses: List[str] = Field(..., description=\"List of specific structural issues (e.g., 'Sudden topic change in Para 3').\")\n",
    "\n",
    "class LogicAnalysisResult(BaseModel):\n",
    "    relevance: RelevanceAnalysis = Field(..., description=\"Analysis of how well the essay answers the prompt.\")\n",
    "    structure: StructureAnalysis = Field(..., description=\"Analysis of the essay's organization.\")\n",
    "    identified_fallacies: List[LogicalFallacy] = Field(default_factory=list, description=\"List of logical errors found in the argumentation.\")\n",
    "    argument_strength_score: int = Field(..., description=\"Score 1-10 on the overall persuasiveness and soundness of arguments.\")\n",
    "    summary_critique: str = Field(..., description=\"A concise summary of the logical quality for the final grader.\")\n",
    "\n",
    "# --- Step 2: Setup the Model (DeepSeek via OpenAI API) ---\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-ai/DeepSeek-V3\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    openai_api_base=\"https://api.siliconflow.cn/v1\",\n",
    "    temperature=0 \n",
    ")\n",
    "\n",
    "structured_llm = llm.with_structured_output(LogicAnalysisResult)\n",
    "\n",
    "# --- Step 3: Refined \"Strict Editor\" Prompt ---\n",
    "system_prompt = \"\"\"\n",
    "You are a strict Academic Journal Editor and Logic Expert.\n",
    "Your task is to ruthlessly evaluate the **Relevance** and **Logic** of the student's essay against the provided Question.\n",
    "\n",
    "**Analysis Directives:**\n",
    "1. **Relevance is Paramount:** If the essay is well-written but answers the wrong question, it must receive a low Relevance Score and `is_off_topic=True`.\n",
    "2. **Logical Rigor:** Hunt for logical fallacies. Does the conclusion follow from the premises? Are the claims supported by evidence or just asserted?\n",
    "3. **Structure:** Check for standard academic form (Intro -> Body Arguments -> Conclusion).\n",
    "\n",
    "Output strictly in the requested JSON format.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"Essay Question: {question}\\n\\nStudent Essay Content:\\n{essay_content}\")\n",
    "])\n",
    "\n",
    "chain = prompt | structured_llm\n",
    "\n",
    "# --- Step 4: Execution Logic ---\n",
    "\n",
    "def run_logic_check(pdf_path: str, essay_question: str):\n",
    "    print(f\"Loading PDF from: {pdf_path}\")\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    if not docs:\n",
    "        print(\"Error: No documents found.\")\n",
    "        return None\n",
    "\n",
    "    # Merge full text for comprehensive logical analysis\n",
    "    full_text = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "    \n",
    "    print(f\"Analyzing Logic against Question: '{essay_question[:50]}...'\")\n",
    "    \n",
    "    try:\n",
    "        # Pass both the QUESTION and the CONTENT\n",
    "        result = chain.invoke({\n",
    "            \"question\": essay_question, \n",
    "            \"essay_content\": full_text\n",
    "        })\n",
    "        \n",
    "        return result.dict()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during logic analysis: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Usage Example ---\n",
    "pdf_path = r\"C:\\Users\\HP\\Documents\\repos\\essay-checker-agentic-rag\\data\\raw\\essay_content.pdf\"\n",
    "question = \"Discuss the socio-economic impacts of the Industrial Revolution in 19th-century Britain.\"\n",
    "\n",
    "result_json = run_logic_check(pdf_path, question)\n",
    "\n",
    "if result_json:\n",
    "    import json\n",
    "    print(json.dumps(result_json, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
