{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ac1f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install dependencies and import libraries\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb67f528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ea4ac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a standard ChatModel with a high context window.\n",
    "# We do NOT use structured output here because you want a Markdown report.\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-ai/DeepSeek-V3\", # or \"gpt-4o\"\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    openai_api_base=\"https://api.siliconflow.cn/v1\",\n",
    "    temperature=0.2 # Slight creativity for feedback writing, but low for consistency\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fab83588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Refined System Prompt ---\n",
    "system_prompt = \"\"\"\n",
    "You are the **Lead Academic Examiner** for an advanced university course.\n",
    "Your task is to grade a student essay by strictly synthesizing data from three expert AI sub-agents (Logic, Fact, Language) and applying a specific Grading Rubric.\n",
    "\n",
    "### 1. INPUT DATA OVERVIEW\n",
    "You will receive:\n",
    "1.  **Rubric:** The exact criteria and band descriptors.\n",
    "2.  **Logic Report:** Scores on relevance, structure, and argument strength.\n",
    "3.  **Fact Report:** Verification of claims and citations.\n",
    "4.  **Language Report:** Analysis of grammar, vocabulary, and tone.\n",
    "5.  **Student Essay:** The raw text.\n",
    "\n",
    "### 2. GRADING ALGORITHM (Mental Steps)\n",
    "Before writing the report, perform this analysis:\n",
    "* **Step 1 (Relevance Check):** Look at `logic_analysis['relevance']['is_off_topic']`. If TRUE, the maximum score for \"Task Response\" is capped at **Band 5**.\n",
    "* **Step 2 (Map Evidence to Rubric):**\n",
    "    * *Task Response:* Use `logic_analysis['relevance']` and `logic_analysis['argument_strength_score']`.\n",
    "    * *Cohesion/Structure:* Use `logic_analysis['structure']['flow_score']` and `language_analysis['structure']['flow_issues']`.\n",
    "    * *Language/Style:* Use `language_analysis['grammar_issues']` count and `language_analysis['vocabulary']['score']`.\n",
    "    * *Evidence/Referencing:* Use `fact_checking_output` (look for incorrect citations) and `logic_analysis['identified_fallacies']`.\n",
    "* **Step 3 (Select Band):** For each criterion, find the Rubric Level where the `descriptor_points` best match your analysis.\n",
    "\n",
    "### 3. OUTPUT RULES\n",
    "* **Tone:** Professional, constructive, and authoritative.\n",
    "* **Justification:** In the Scorecard, you MUST quote specific **Descriptor Points** from the rubric to justify the score (e.g., \"Matches Band 7: 'Argument is clear but may lack refinement'\").\n",
    "* **Annotations:** In the \"Annotated Text Review\", you must strictly use the JSON data to pinpoint errors. Do not hallucinate new errors.\n",
    "\n",
    "### 4. FINAL OUTPUT FORMAT\n",
    "Produce a clean Markdown report strictly following the template below.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "404f8cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Prompt Template\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"\"\"\n",
    "**ESSAY QUESTION:** {essay_question}\n",
    "\n",
    "**RUBRIC:** {rubric_json}\n",
    "\n",
    "**LOGIC & RELEVANCE REPORT:** {logic_json}\n",
    "\n",
    "**FACT CHECK REPORT:** {fact_json}\n",
    "\n",
    "**LANGUAGE REPORT:** {language_json}\n",
    "\n",
    "**STUDENT ESSAY:** {essay_content}\n",
    "\n",
    "---\n",
    "Generate the **Academic Assessment Report** now.\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "# Create the Chain\n",
    "grading_chain = prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09913c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Execution Function ---\n",
    "def generate_final_grade(\n",
    "    essay_content: str,\n",
    "    essay_question: str,\n",
    "    rubric_data: dict,\n",
    "    logic_data: dict,\n",
    "    fact_data: list,\n",
    "    language_data: dict\n",
    "):\n",
    "    print(\"üë©‚Äçüè´ Synthesizing Final Report...\")\n",
    "    \n",
    "    # Convert dicts to JSON strings for the prompt \n",
    "    # (Explicit JSON Serialization) using json.dumps\n",
    "    try:\n",
    "        report = grading_chain.invoke({\n",
    "            \"essay_question\": essay_question,\n",
    "            \"essay_content\": essay_content,\n",
    "            \"rubric_json\": json.dumps(rubric_data, indent=2),\n",
    "            \"logic_json\": json.dumps(logic_data, indent=2),\n",
    "            \"fact_json\": json.dumps(fact_data, indent=2),\n",
    "            \"language_json\": json.dumps(language_data, indent=2)\n",
    "        })\n",
    "        return report\n",
    "    except Exception as e:\n",
    "        return f\"Error generating report: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ab85970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON files loaded successfully.\n",
      "PDF files loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# load all data from previous functions\n",
    "#unserialized JSON data using json.load\n",
    "\n",
    "EXTRACTED_RUBRICS_PATH = \"C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\processed\\\\extracted_rubrics.json\"\n",
    "FACT_CHECKING_OUTPUT_PATH = \"C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\processed\\\\fact_checking_output.json\"\n",
    "LOGIC_ANALYSIS_OUTPUT_PATH = \"C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\processed\\\\logic_analysis_output.json\"\n",
    "LANGUAGE_ANALYSIS_OUTPUT_PATH = \"C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\processed\\\\language_analysis_output.json\"\n",
    "\n",
    "with open(EXTRACTED_RUBRICS_PATH, \"r\") as f:\n",
    "    extracted_rubrics = json.load(f)\n",
    "\n",
    "with open(FACT_CHECKING_OUTPUT_PATH, \"r\") as f:\n",
    "    fact_checking_output = json.load(f)\n",
    "\n",
    "with open(LOGIC_ANALYSIS_OUTPUT_PATH, \"r\") as f:\n",
    "    logic_analysis_output = json.load(f)\n",
    "\n",
    "with open(LANGUAGE_ANALYSIS_OUTPUT_PATH, \"r\") as f:\n",
    "    language_analysis_output = json.load(f)\n",
    "\n",
    "print(\"JSON files loaded successfully.\")\n",
    "\n",
    "ESSAY_QUESTION_PATH = \"C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\raw\\\\essay_question.pdf\"\n",
    "ESSAY_CONTENT_PATH = \"C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\raw\\\\essay_content.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(ESSAY_QUESTION_PATH)\n",
    "docs = loader.load()\n",
    "essay_question = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "loader = PyPDFLoader(ESSAY_CONTENT_PATH)\n",
    "docs = loader.load()\n",
    "essay_content = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "print(\"PDF files loaded successfully.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e991261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë©‚Äçüè´ Synthesizing Final Report...\n",
      "# Academic Assessment Report\n",
      "\n",
      "## Scorecard Summary\n",
      "\n",
      "| Criteria                      | Band Score | Weighted Score | Justification |\n",
      "|-------------------------------|------------|----------------|---------------|\n",
      "| Task Response & Argument      | Band 9     | 20%            | Matches Band 9: \"Fully addresses all parts of the task with a clear, insightful, and original argument\" and \"Demonstrates a nuanced understanding of GenAI's challenges and benefits\" |\n",
      "| Critical Thinking & Evaluation| Band 9     | 20%            | Matches Band 9: \"Demonstrates excellent critical engagement with sources and ideas\" and \"Evaluates implications, limitations, and counterarguments insightfully\" |\n",
      "| Use of Sources & Referencing  | Band 8     | 20%            | Matches Band 8: \"Uses an appropriate range of sources effectively\" and \"Referencing is largely correct with occasional minor errors\" |\n",
      "| Academic Writing Style & Register | Band 8 | 15%         | Matches Band 8: \"Appropriate academic register throughout with strong lexical choice\" |\n",
      "| Organisation & Cohesion       | Band 9     | 15%            | Matches Band 9: \"Exceptionally well-structured with clear progression of ideas\" and \"Paragraphing is logical and cohesive\" |\n",
      "| Language Accuracy & Control   | Band 8     | 10%            | Matches Band 8: \"High level of grammatical accuracy with only occasional minor errors that do not impede meaning\" |\n",
      "\n",
      "**Total Score: 92% (Distinction)**\n",
      "\n",
      "## Detailed Evaluation\n",
      "\n",
      "### Strengths\n",
      "\n",
      "1. **Argument Development (Band 9)**\n",
      "   - The essay presents a sophisticated and balanced argument that fully addresses all aspects of the prompt. The position on university adoption is consistently sustained throughout, moving beyond simplistic prohibition to advocate for critical integration.\n",
      "\n",
      "2. **Critical Analysis (Band 9)**\n",
      "   - Demonstrates excellent engagement with both challenges (academic integrity) and benefits (personalized learning), culminating in a persuasive call for assessment reform. The analysis shows independent thinking by connecting pedagogical implications with workplace readiness.\n",
      "\n",
      "3. **Structure and Flow (Band 9)**\n",
      "   - The essay follows a clear logical progression from challenges to benefits to solutions. Topic sentences effectively signpost content, and transitions between sections are well-managed with only minor abruptness noted.\n",
      "\n",
      "### Areas for Improvement\n",
      "\n",
      "1. **Referencing Consistency (Band 8)**\n",
      "   - While sources are used effectively, there are minor inconsistencies in citation format (e.g., \"EduTech Future, 2023\" vs. standard academic citation style). Ensure all references follow ESAC conventions precisely.\n",
      "\n",
      "2. **Language Precision (Band 8)**\n",
      "   - Minor grammatical inconsistencies were noted (hyphenation in \"standard essay-writing\" and British/American spelling variations). Maintaining consistency in these areas would enhance professionalism.\n",
      "\n",
      "3. **Transitions (Band 8)**\n",
      "   - Some transitions between paragraphs could be smoother. Adding brief bridging sentences would further strengthen the cohesive flow.\n",
      "\n",
      "## Annotated Text Review\n",
      "\n",
      "1. **Grammar/Mechanics**\n",
      "   - \"synthesise\" ‚Üí \"synthesize\" (Line 5): Spelling consistency issue (British vs. American English)\n",
      "   - \"standard essay writing\" ‚Üí \"standard essay-writing\" (Body Paragraph 1): Missing hyphen in compound adjective\n",
      "\n",
      "2. **Structure Feedback**\n",
      "   - Transition between Body Paragraphs 1 and 2 could be enhanced with a bridging sentence to connect integrity challenges to learning benefits more explicitly.\n",
      "\n",
      "3. **Source Use**\n",
      "   - All factual claims were verified as accurate (e.g., ChatGPT as GenAI tool, academic integrity challenges). Sources are well-paraphrased with no direct quotations.\n",
      "\n",
      "## Final Recommendation\n",
      "\n",
      "This is an outstanding essay that demonstrates sophisticated understanding of GenAI's dual role in higher education. The argument is original, well-supported, and professionally presented. With minor refinements in referencing consistency and transitional phrasing, this work approaches publishable quality. The student shows exceptional ability to synthesize complex ideas and propose innovative solutions to contemporary educational challenges.\n"
     ]
    }
   ],
   "source": [
    "final_report = generate_final_grade(\n",
    "    essay_content=essay_content,\n",
    "    essay_question=essay_question,\n",
    "    rubric_data=extracted_rubrics, # Your variable\n",
    "    logic_data=logic_analysis_output, # Your variable\n",
    "    fact_data=fact_checking_output, # Your variable\n",
    "    language_data=language_analysis_output # Your variable\n",
    ")\n",
    "    \n",
    "print(final_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16814101",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORT_OUTPUT_PATH = \"C:\\\\Users\\\\HP\\\\Documents\\\\repos\\\\essay-checker-agentic-rag\\\\data\\\\final_report\\\\final_report.txt\"\n",
    "\n",
    "with open(REPORT_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378058c5",
   "metadata": {},
   "source": [
    "## Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0164fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# --- Setup the Model (DeepSeek/GPT-4o) ---\n",
    "# We use a standard ChatModel with a high context window.\n",
    "# We do NOT use structured output here because you want a Markdown report.\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-ai/DeepSeek-V3\", # or \"gpt-4o\"\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    openai_api_base=\"https://api.siliconflow.cn/v1\",\n",
    "    temperature=0.2 # Slight creativity for feedback writing, but low for consistency\n",
    ")\n",
    "\n",
    "# --- Refined System Prompt ---\n",
    "system_prompt = \"\"\"\n",
    "You are the **Lead Academic Examiner** for an advanced university course.\n",
    "Your task is to grade a student essay by strictly synthesizing data from three expert AI sub-agents (Logic, Fact, Language) and applying a specific Grading Rubric.\n",
    "\n",
    "### 1. INPUT DATA OVERVIEW\n",
    "You will receive:\n",
    "1.  **Rubric:** The exact criteria and band descriptors.\n",
    "2.  **Logic Report:** Scores on relevance, structure, and argument strength.\n",
    "3.  **Fact Report:** Verification of claims and citations.\n",
    "4.  **Language Report:** Analysis of grammar, vocabulary, and tone.\n",
    "5.  **Student Essay:** The raw text.\n",
    "\n",
    "### 2. GRADING ALGORITHM (Mental Steps)\n",
    "Before writing the report, perform this analysis:\n",
    "* **Step 1 (Relevance Check):** Look at `logic_analysis['relevance']['is_off_topic']`. If TRUE, the maximum score for \"Task Response\" is capped at **Band 5**.\n",
    "* **Step 2 (Map Evidence to Rubric):**\n",
    "    * *Task Response:* Use `logic_analysis['relevance']` and `logic_analysis['argument_strength_score']`.\n",
    "    * *Cohesion/Structure:* Use `logic_analysis['structure']['flow_score']` and `language_analysis['structure']['flow_issues']`.\n",
    "    * *Language/Style:* Use `language_analysis['grammar_issues']` count and `language_analysis['vocabulary']['score']`.\n",
    "    * *Evidence/Referencing:* Use `fact_checking_output` (look for incorrect citations) and `logic_analysis['identified_fallacies']`.\n",
    "* **Step 3 (Select Band):** For each criterion, find the Rubric Level where the `descriptor_points` best match your analysis.\n",
    "\n",
    "### 3. OUTPUT RULES\n",
    "* **Tone:** Professional, constructive, and authoritative.\n",
    "* **Justification:** In the Scorecard, you MUST quote specific **Descriptor Points** from the rubric to justify the score (e.g., \"Matches Band 7: 'Argument is clear but may lack refinement'\").\n",
    "* **Annotations:** In the \"Annotated Text Review\", you must strictly use the JSON data to pinpoint errors. Do not hallucinate new errors.\n",
    "\n",
    "### 4. FINAL OUTPUT FORMAT\n",
    "Produce a clean Markdown report strictly following the template below.\n",
    "\"\"\"\n",
    "\n",
    "# Define the Prompt Template\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"\"\"\n",
    "**ESSAY QUESTION:** {question}\n",
    "\n",
    "**RUBRIC:** {rubric_json}\n",
    "\n",
    "**LOGIC & RELEVANCE REPORT:** {logic_json}\n",
    "\n",
    "**FACT CHECK REPORT:** {fact_json}\n",
    "\n",
    "**LANGUAGE REPORT:** {language_json}\n",
    "\n",
    "**STUDENT ESSAY:** {essay_text}\n",
    "\n",
    "---\n",
    "Generate the **Academic Assessment Report** now.\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "# Create the Chain\n",
    "grading_chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "# --- Execution Function ---\n",
    "def generate_final_grade(\n",
    "    essay_text: str,\n",
    "    question: str,\n",
    "    rubric_data: dict,\n",
    "    logic_data: dict,\n",
    "    fact_data: list,\n",
    "    language_data: dict\n",
    "):\n",
    "    print(\"üë©‚Äçüè´ Synthesizing Final Report...\")\n",
    "    \n",
    "    # Convert dicts to JSON strings for the prompt\n",
    "    try:\n",
    "        report = grading_chain.invoke({\n",
    "            \"question\": question,\n",
    "            \"essay_text\": essay_text,\n",
    "            \"rubric_json\": json.dumps(rubric_data, indent=2),\n",
    "            \"logic_json\": json.dumps(logic_data, indent=2),\n",
    "            \"fact_json\": json.dumps(fact_data, indent=2),\n",
    "            \"language_json\": json.dumps(language_data, indent=2)\n",
    "        })\n",
    "        return report\n",
    "    except Exception as e:\n",
    "        return f\"Error generating report: {e}\"\n",
    "\n",
    "# --- Example Usage (Using your provided dummy data) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your raw data (Simulated here based on your prompt)\n",
    "    # in a real app, these come from the previous functions\n",
    "    \n",
    "    # ... (Load your JSONs here) ...\n",
    "    \n",
    "    final_report = generate_final_grade(\n",
    "        essay_text=\"Generative Artificial Intelligence (GenAI) tools, such as ChatGPT...\", # Truncated\n",
    "        question=\"Discuss the integration of GenAI in higher education.\",\n",
    "        rubric_data=extracted_rubrics, # Your variable\n",
    "        logic_data=logic_analysis_output, # Your variable\n",
    "        fact_data=fact_checking_output, # Your variable\n",
    "        language_data=language_analysis_output # Your variable\n",
    "    )\n",
    "    \n",
    "    print(final_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
